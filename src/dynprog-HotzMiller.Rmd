# Dynamic Programming

## A simple numerical model

Let's consider a simple dynamic programing problem. In this problem set we want to simulate from a single agent problem and use the Hotz and Miller approach to recover the parameters.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(RcppSimpleTensor)
require(plyr)
require(ggplot2)
require(data.table)
require(reshape2)
source("utils.r")
```

### Setting the environment

Let's use a very simple model that we can solve exactly. The state space will be discrete, we will consider a two sector problem and staying at thome. We will use Rust assumptions. For simplicity we consider only 10 years.

We consider the following preferences:

$$ U(w,a,\xi) = \gamma \cdot \frac{w^{1-\rho}}{1-\rho} + u(a) +  \xi(a)$$
where $\xi(a)$ is an iid, type 1 extreme value shock, and $u(a)$ is a permanent preference vector. We consider the following wage equation:

$$ \log(w_{it}(a)) = \beta_a E_{it} + \beta_r t +  \epsilon_{it}(a) $$
where $E_{it}$ is a stochastic observed experience value. $\epsilon(a)$ is iid and normaly distributed. The agent does not know the $\epsilon(a)$ at the time where she decides which action $a$ to choose. As we know we can solve for the expected value function given by:

$$ \bar{V}_t(x) = \log \sum_a \exp \Big[  \mathbb{E} U(w,a,\xi) + \beta \sum_{x_{t+1}}\bar{V}_t(x_{t+1})) g(x_{t+1}|a) \Big]$$
because of the log-normal shocks we get that 
$$\mathbb{E} U(w,a,\xi) =   \gamma  \cdot \frac{ \exp \Big( (1-\rho)^2 \sigma_\epsilon^2/2) \Big) }{1-\rho} \exp\Big\{ (1-\rho) \cdot (\beta_a E_{it} + \beta_r t)\Big\} + u(a) +  \xi(a) .$$
## Preparing the environment

We set parameters and initialize the transition matrices, value functions and Q-values.

```{r}
# setting paramters
p    = list(nt=10, # nb of periods
            ne=10, # nb of experience level
            rho=2, # risk aversion
            wsd=1, # standard deviation of wage shock epsilon
            r1=0.1,r2=0,rt=0.05,   # return to experience in activity 1 and 2, and age effect 
            u0=0,u1=3,u2=2.5,  # preference values for each choice
            r=0.02,
            gamma=1.2) # discount paramter

# init value function
V    = array(0,c(p$nt,p$ne))
# init Q-value
QQa = array(0,c(p$nt,p$ne,3))

# prepare levels for experience
E = 1:p$ne

# construct transition matrix for each choice
trm <- function(d,s,n) {
  X1 = spread(qnorm( (1:n)/(n+1) ),1,n)
  D = dnorm( X1 - (t(X1) +d),0,s)
  D = D / spread(rowSums(D),2,n) 
}

# global transition matrix
GG = array(0,c(3,p$ne,p$ne))
GG[1,,] = trm(-0.2,0.2,p$ne)  # when not working, epxerience depreciates
GG[2,,] = trm(0   ,0.2,p$ne)  # in activity 2, experience does not accumulate
GG[3,,] = trm(1   ,0.2,p$ne)  # in activity 3, experience accumulates

# define utility function
uf <- function(w,p) p$gamma*(w^(1-p$rho))/(1-p$rho)
```

We then solve the dynamic problem recursively. In the last period they just get the last wage for ever.

```{r final}
S = exp( p$wsd^2 * (1-p$rho)^2/2) # scaling due to the shock

# final values (get the same choice for ever)
Vf = S*cbind( p$u0 , uf(p$r1 * E + p$rt*p$nt ,p) +p$u1, uf(p$r2 * E + p$rt*p$nt ,p) + p$u2)/p$r
V[p$nt,] = log( rowSums(exp(Vf)))
QQa[p$nt,,] = Vf

```

## Solving dynamic problem

We then solve the values recursively
```{r solving}
# construct Q value for each choice of the 3 choices
for (t in (p$nt-1):1) {
  Q0 =                                p$u0  + 1/(1+p$r)* GG[1,,] %*% V[t+1,]
  Q1 =  S*uf(p$r1 * E + p$rt*p$nt ,p) + p$u1  + 1/(1+p$r)* GG[2,,] %*% V[t+1,]
  Q2 =  S*uf(p$r2 * E + p$rt*p$nt ,p) + p$u2  + 1/(1+p$r)* GG[3,,] %*% V[t+1,]
  QQ = cbind(Q0,Q1,Q2)
  V[t,] = log(rowSums(exp(QQ)))
  
  # saving the choice probability
  QQa[t,,] = QQ
}
```

## Simulating

Then we simulate a data set:

```{r simulating,fig.align="center"}
N=50000
simdata = data.table(ii=1:N)
R = c(0,p$r1,p$r2)

dE = 1:10
dW = 1:10
dA = 1:10

simdata = simdata[,{
  dE[1] = sample.int(p$ne,1,prob=1/(1:p$ne))
  for (t in 1:p$nt) {
    pr = exp(QQa[t,dE[1],] - max(QQa[t,dE[1],]))
    dA[t] = sample.int(3,1,prob= exp(QQa[t,dE[1],]))
    dW[t] = exp(R[dA[t]] * dE[t] + p$rt*t + p$wsd*rnorm(1))
    if (dA[t]==1) dW[t]=NA;
    if (t<p$nt) {dE[t+1] = sample.int(p$ne,1,prob=GG[dA[t],dE[t],])}
  }
  list(A=dA,W=dW,E=dE,t=1:p$nt)
},ii]

rr = simdata[,.N,list(A,t)][,pr:=N/sum(N),t]
ggplot(rr[t<10],aes(x=t,y=pr,color=factor(A))) + geom_line() +ggtitle("Average choices over time") + theme_bw()

rr = simdata[,list(m=mean(E),q975=quantile(E,0.975),q025=quantile(E,0.025)),t]
ggplot(rr,aes(x=t,y=m,ymin=q025,ymax=q975)) + geom_line() + geom_errorbar(width=0.2) +ggtitle("value of E over time") + theme_bw()
```

## Questions

We want to apply the Hotz and Miller approach to estimate this dynamic programing problem.

### **Q1** Payoff equation and transition function

Show that you can recover direclty the parameter of the wage equation in this problem. Write the regression code that recovers them. Also write code that recovers the transition matrices $G_0,G_1,G_2$. Show that the $G$ estimated matrices align with the true one. Then do a parametric estimation by imposing the known form for the transition and recover the slope and variance parameters use for each $a$ in the first part.

### **Q2** CCP given risk aversion and $r=\infty$

We start with a simple case where there is full discounting. Express $\log(Pr[a=2|E,t,W] / Pr[a=1|E,t,W])$ and $\log(Pr[a=3|E,t,W] / Pr[a=1|E,t])$. Write down an estimator (which is linear) for $u(a=2)$ and $U(a=3)$ in this case and show that you can get the values from simulated data.

### **Q3** CCP given risk aversion

The risk aversion coefficient does not enter linearly in the utility problem. We are going to fix it, and use Hotz and Miller conditional on that value. So Assume that $\rho$ is known and fixed at some value. We need to construct our expressions that will help us recover $u(a=2)$ and $U(a=3)$ in this dynamic case. This requires the use of the Euler cosntant $\gamma_e = 0.577$.

Start by computing the choice probabilities in the simulated data $Pr[a|E,t]$. Why do we not need to also condition on the wage?

From these conditional choice probablities, and using the know value of $\rho$, construct the coefficients $\alpha_{t}(a|E)$ and $\beta_{t}(a,E)$ such that:

$$ 
\begin{align*}
Q_t(a|E) &= E U(w,a,\xi) + \beta \sum_{x_{t+1}}\bar{V}_t(x_{t+1})) g(x_{t+1}|a) + \xi(a) \\
         &= \alpha_{t}(a|E) + \beta_{t}(a,E) \cdot u(a) \\
\end{align*}
$$

These are the analog of $\tilde{r} and \tilde{e}$ that we covered in class. Note that this values have to use  Give this expressions, construct a linear expression using log probabilty choices that recovers the $u(a)$ values.

### **Q4** Full estimation

The final step is then to put the whole previous procedure inside a function that takes $\rho$ as an argument and returns the fit of the last step that recovers the $u(a)$ values. Evaluate the fit on a grid of values for $\rho$ and return that plot.

 
 
 
 
 


